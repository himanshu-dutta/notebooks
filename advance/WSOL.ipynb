{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms.transforms import Lambda\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as tfrms\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "# WSOL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "\n",
    "        return F.relu(x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stem(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            *(list(torchvision.models.resnet50(pretrained=True).children())[:1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.a1 = nn.Sequential(\n",
    "            ConvBlock(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            ConvBlock(64, 64, kernel_size=3),\n",
    "            ConvBlock(64, 128, kernel_size=3, padding=1),\n",
    "            ConvBlock(128, 128, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.a2 = nn.Sequential(\n",
    "            ConvBlock(128, 128, kernel_size=3, stride=2),\n",
    "            ConvBlock(128, 256, kernel_size=3),\n",
    "            ConvBlock(256, 256, kernel_size=3),\n",
    "            ConvBlock(256, 1024, kernel_size=3),\n",
    "            ConvBlock(1024, 1024, kernel_size=3),\n",
    "        )\n",
    "\n",
    "        self.a3 = nn.Sequential(\n",
    "            ConvBlock(1024, 1024, kernel_size=3, padding=1),\n",
    "            ConvBlock(1024, 2048, kernel_size=3),\n",
    "        )\n",
    "\n",
    "        self.a4 = ConvBlock(2048, num_classes, kernel_size=3)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_a1 = self.a1(x)\n",
    "        out_a2 = self.a2(out_a1)\n",
    "        out_a3 = self.a3(out_a2)\n",
    "        out_a4 = self.a4(out_a3)\n",
    "\n",
    "        out = self.gap(out_a4)\n",
    "\n",
    "        return (out_a1, out_a2, out_a3, out_a4, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class B(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1x = ConvBlock(128, 64, kernel_size=7, stride=3)\n",
    "        self.conv1y = ConvBlock(1024, 64, kernel_size=3)\n",
    "\n",
    "        self.conv2 = ConvBlock(64, 64, kernel_size=3)\n",
    "        self.conv3 = ConvBlock(64, num_classes, kernel_size=3, padding=1)\n",
    "\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.conv1x(x)\n",
    "        y = self.conv1y(y)\n",
    "\n",
    "        x = self.conv3(self.conv2(x))\n",
    "        y = self.conv3(self.conv2(y))\n",
    "\n",
    "        return (self.sig(x), self.sig(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(2048, 1024, kernel_size=3)\n",
    "        self.conv2 = ConvBlock(1024, num_classes, kernel_size=3, padding=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.sig(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSOL(nn.Module):\n",
    "    def __init__(self, num_classes, threshold=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.thres = threshold\n",
    "\n",
    "        self.stem = Stem()\n",
    "        self.a = A(num_classes=num_classes)\n",
    "        self.b = B(num_classes=num_classes)\n",
    "        self.c = C(num_classes=num_classes)\n",
    "\n",
    "        self.upsample = nn.Upsample((240, 240))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(), nn.Linear(in_features=19 * 16 * 16, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = self.stem(x)\n",
    "\n",
    "        # A\n",
    "        [out_a1, out_a2, out_a3, out_a4, gap] = self.a(feature_map)\n",
    "\n",
    "        # B\n",
    "        [out_b1, out_b2] = self.b(out_a1, out_a2)\n",
    "\n",
    "        # C\n",
    "        out_c = self.c(out_a3)\n",
    "\n",
    "        # logits\n",
    "        logits = self.fc(gap)\n",
    "\n",
    "        return {\n",
    "            \"a\": self.upsample(out_a4),\n",
    "            \"b1\": self.upsample(out_b1),\n",
    "            \"b2\": self.upsample(out_b2),\n",
    "            \"c\": self.upsample(out_c),\n",
    "            \"logits\": logits,\n",
    "        }\n",
    "\n",
    "    def train_step(self, xb):\n",
    "        images, labels = xb\n",
    "        out = self(images)\n",
    "\n",
    "        a = self.apply_threshold(out[\"a\"])\n",
    "        b1 = out[\"b1\"]\n",
    "        b2 = self.apply_threshold(out[\"b2\"])\n",
    "        c = out[\"c\"]\n",
    "\n",
    "        loss_ab = self.loss_saliency(F.binary_cross_entropy_with_logits, a, out[\"b2\"])\n",
    "        loss_bb = self.loss_saliency(F.binary_cross_entropy_with_logits, b2, b1)\n",
    "        loss_bc = self.loss_saliency(F.binary_cross_entropy_with_logits, b2, c)\n",
    "\n",
    "        loss_logits = F.cross_entropy(out[\"logits\"], labels)\n",
    "\n",
    "        loss = loss_ab + loss_bb + loss_bc + loss_logits\n",
    "\n",
    "        return {\"loss\": loss, \"map\": [a, b2]}\n",
    "\n",
    "    def apply_threshold(self, x):\n",
    "        bg = x < self.thres\n",
    "        mask = x > self.thres\n",
    "        x[bg.data] = 0.0\n",
    "        x[mask.data] = 1.0\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss_saliency(self, loss_func, logits, labels):\n",
    "        positions = labels.view(-1, 1) < 255.0\n",
    "\n",
    "        return loss_func(\n",
    "            logits.view(logits.shape[0], -1), labels.view(labels.shape[0], -1)\n",
    "        )"
   ]
  },
  {
   "source": [
    "# Execution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WSOL(num_classes=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(images, means, stds, channels):\n",
    "    means = torch.tensor(means).reshape(1, channels, 1, 1)\n",
    "    stds = torch.tensor(stds).reshape(1, channels, 1, 1)\n",
    "    return images * stds + means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(images):\n",
    "    fig, ax = plt.subplots(figsize=(24, 24))\n",
    "    for img in images:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        img_ = img.detach().clamp(0, 1).numpy()\n",
    "        print(img_.shape)\n",
    "        ax.imshow(img_)\n",
    "        plt.show()\n",
    "        input()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrm = tfrms.Compose(\n",
    "    [\n",
    "        tfrms.Resize((240, 240)),\n",
    "        tfrms.ToTensor(),\n",
    "        tfrms.Lambda(lambda x: x.to(torch.float32)),\n",
    "        tfrms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),\n",
    "        tfrms.Normalize(0.445, 0.225),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"./test_images/img1.png\")\n",
    "sample_inp = torch.unsqueeze(tfrm(img), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_inp = torch.rand((2, 3, 240, 240))\n",
    "sample_label = torch.rand((1,)).to(torch.long)\n",
    "sample_out = model.train_step((sample_inp, sample_label))\n",
    "\n",
    "print(\"loss\", sample_out[\"loss\"].shape)\n",
    "print(\"a\", sample_out[\"map\"][0].shape)\n",
    "print(\"b\", sample_out[\"map\"][1].shape)\n",
    "\n",
    "\n",
    "maps = denormalize(sample_out[\"map\"][0], (0.445,) * 19, (0.225,) * 19, 19)\n",
    "\n",
    "\n",
    "# masking the input image\n",
    "masks = maps[0].detach().numpy()\n",
    "img_arr = np.array(img.resize((240, 240)))\n",
    "\n",
    "final_mask = masks[0]\n",
    "for mask in masks[1:]:\n",
    "    final_mask += mask\n",
    "\n",
    "masked_img = img_arr * mask\n",
    "masked_img = Image.fromarray(masked_img).convert(\"RGB\")\n",
    "masked_img.save(f\"./outputs/masked_combined.png\")\n",
    "\n",
    "\n",
    "for idx, mask in enumerate(masks):\n",
    "    masked_img = img_arr * mask\n",
    "    masked_img = Image.fromarray(masked_img).convert(\"RGB\")\n",
    "    # masked_img.show()\n",
    "    masked_img.save(f\"./outputs/masked_{idx}.png\")"
   ]
  }
 ]
}