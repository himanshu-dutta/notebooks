{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd01811777d830f1030e31060b656d737abbce627438427bf59fb8b24dc91025654",
   "display_name": "Python 3.8.5 64-bit ('.env': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "1811777d830f1030e31060b656d737abbce627438427bf59fb8b24dc91025654"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "import utils.data as data"
   ]
  },
  {
   "source": [
    "# MLP Mixer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, expansion):\n",
    "        super(MLP, self).__init__()\n",
    "        self.lin1 = nn.Linear(dim, dim * expansion)\n",
    "        self.act = nn.GELU()\n",
    "        self.lin2 = nn.Linear(dim * expansion, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        return self.lin2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixerLayer(nn.Module):\n",
    "    def __init__(self, num_patches, num_channels, expansion):\n",
    "        super(MixerLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(num_channels)\n",
    "        self.by_patch = MLP(num_patches, expansion)\n",
    "        self.by_channel = MLP(num_channels, expansion)\n",
    "        self.norm2 = nn.LayerNorm(num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape -> B, P, C\n",
    "        identity = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # x.shape -> B, C, P\n",
    "        x = self.by_patch(torch.transpose(x, 1, 2))\n",
    "\n",
    "        # x.shape -> B, P, C\n",
    "        x = torch.transpose(x, 1, 2) + identity\n",
    "\n",
    "        # x.shape -> B, P, C\n",
    "        identity = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.by_channel(x) + identity\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMixer(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_sz,\n",
    "        img_channels,\n",
    "        num_classes,\n",
    "        depth,\n",
    "        num_patches,\n",
    "        num_channels,\n",
    "        expansion,\n",
    "    ):\n",
    "        super(MLPMixer, self).__init__()\n",
    "        self.img_sz = img_sz\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.patch_sz = int(((self.img_sz ** 2) // self.num_patches) ** (1 / 2))\n",
    "\n",
    "        inp_channels = ((img_sz ** 2) // num_patches) * img_channels\n",
    "\n",
    "        self.per_patch = nn.Linear(inp_channels, num_channels)\n",
    "\n",
    "        self.mixer_layers = nn.ModuleList(\n",
    "            [MixerLayer(num_patches, num_channels, expansion) for _ in range(depth)]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(num_channels, num_classes)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        bs = x.shape[0]\n",
    "\n",
    "        x = (\n",
    "            x.data.unfold(1, self.img_channels, self.img_channels)\n",
    "            .unfold(2, self.patch_sz, self.patch_sz)\n",
    "            .unfold(3, self.patch_sz, self.patch_sz)\n",
    "        )\n",
    "\n",
    "        x = x.reshape(bs, -1, self.img_channels * self.patch_sz * self.patch_sz)\n",
    "\n",
    "        x = self.per_patch(x)\n",
    "\n",
    "        for layer in self.mixer_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = x.mean(1)\n",
    "\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def training_step(self, xb, batch_idx):\n",
    "        inp, labels = xb\n",
    "        out = self(inp)\n",
    "\n",
    "        return self.loss(out, labels)\n",
    "\n",
    "    def validation_step(self, xb, batch_idx):\n",
    "        inp, labels = xb\n",
    "        out = self(inp)\n",
    "\n",
    "        labels_hat = torch.argmax(out, dim=1)\n",
    "        val_acc = torch.sum(labels == labels_hat).item() / (len(labels) * 1.0)\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            self.loss(out, labels),\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        self.log(\"val_acc\", val_acc, prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "        return val_acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=2e-4)"
   ]
  },
  {
   "source": [
    "# Execution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ize (MB): 5.40\n",
      "Estimated Total Size (MB): 5.90\n",
      "==========================================================================================\n",
      "\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | per_patch    | Linear           | 25.2 K\n",
      "1 | mixer_layers | ModuleList       | 1.3 M \n",
      "2 | classifier   | Linear           | 1.3 K \n",
      "3 | loss         | CrossEntropyLoss | 0     \n",
      "--------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.401     Total estimated model params size (MB)\n",
      "Epoch 0:  86%|████████▌ | 938/1095 [00:44<00:07, 20.86it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  86%|████████▌ | 940/1095 [00:45<00:07, 20.73it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  86%|████████▋ | 945/1095 [00:45<00:07, 20.80it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  87%|████████▋ | 952/1095 [00:45<00:06, 20.90it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  88%|████████▊ | 960/1095 [00:45<00:06, 21.03it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  88%|████████▊ | 968/1095 [00:45<00:06, 21.16it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  89%|████████▉ | 976/1095 [00:45<00:05, 21.28it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  90%|████████▉ | 984/1095 [00:45<00:05, 21.40it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  91%|█████████ | 992/1095 [00:46<00:04, 21.52it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  91%|█████████▏| 1000/1095 [00:46<00:04, 21.64it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  92%|█████████▏| 1008/1095 [00:46<00:03, 21.75it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  93%|█████████▎| 1016/1095 [00:46<00:03, 21.87it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  94%|█████████▎| 1024/1095 [00:46<00:03, 21.98it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  94%|█████████▍| 1032/1095 [00:46<00:02, 22.10it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  95%|█████████▍| 1040/1095 [00:46<00:02, 22.22it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Validating:  65%|██████▍   | 102/157 [00:01<00:00, 68.47it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 1048/1095 [00:46<00:02, 22.33it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  96%|█████████▋| 1056/1095 [00:47<00:01, 22.44it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  97%|█████████▋| 1064/1095 [00:47<00:01, 22.55it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  98%|█████████▊| 1072/1095 [00:47<00:01, 22.67it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  99%|█████████▊| 1080/1095 [00:47<00:00, 22.78it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0:  99%|█████████▉| 1088/1095 [00:47<00:00, 22.89it/s, loss=0.105, v_num=0, val_loss_epoch=2.390, val_acc_epoch=0.0703]\n",
      "Epoch 0: 100%|██████████| 1095/1095 [00:47<00:00, 22.93it/s, loss=0.105, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1:  86%|████████▌ | 938/1095 [00:50<00:08, 18.49it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  86%|████████▌ | 944/1095 [00:51<00:08, 18.41it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Validating:   4%|▍         | 7/157 [00:00<00:09, 15.02it/s]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 952/1095 [00:51<00:07, 18.51it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1:  88%|████████▊ | 960/1095 [00:51<00:07, 18.61it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1:  88%|████████▊ | 968/1095 [00:51<00:06, 18.70it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Validating:  19%|█▉        | 30/157 [00:01<00:03, 39.18it/s]\u001b[A\n",
      "Epoch 1:  89%|████████▉ | 976/1095 [00:51<00:06, 18.80it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1:  90%|████████▉ | 984/1095 [00:52<00:05, 18.90it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Validating:  30%|██▉       | 47/157 [00:01<00:02, 45.59it/s]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 992/1095 [00:52<00:05, 18.99it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1:  91%|█████████▏| 1000/1095 [00:52<00:04, 19.09it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1:  92%|█████████▏| 1008/1095 [00:52<00:04, 19.19it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Validating:  45%|████▌     | 71/157 [00:01<00:01, 50.99it/s]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 1016/1095 [00:52<00:04, 19.28it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1:  94%|█████████▎| 1024/1095 [00:52<00:03, 19.38it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1:  94%|█████████▍| 1032/1095 [00:53<00:03, 19.47it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Validating:  61%|██████    | 95/157 [00:02<00:01, 49.79it/s]\u001b[A\n",
      "Epoch 1:  95%|█████████▍| 1040/1095 [00:53<00:02, 19.55it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1:  96%|█████████▌| 1048/1095 [00:53<00:02, 19.65it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1:  96%|█████████▋| 1056/1095 [00:53<00:01, 19.74it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Validating:  76%|███████▌  | 119/157 [00:02<00:00, 48.96it/s]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 1064/1095 [00:53<00:01, 19.82it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1:  98%|█████████▊| 1072/1095 [00:53<00:01, 19.91it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Validating:  85%|████████▌ | 134/157 [00:03<00:00, 47.58it/s]\u001b[A\n",
      "Epoch 1:  99%|█████████▊| 1080/1095 [00:54<00:00, 20.00it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1:  99%|█████████▉| 1088/1095 [00:54<00:00, 20.09it/s, loss=0.0881, v_num=0, val_loss_epoch=0.103, val_acc_epoch=0.967, val_loss_step=0.00452, val_acc_step=1.000]\n",
      "Epoch 1: 100%|██████████| 1095/1095 [00:54<00:00, 20.11it/s, loss=0.0881, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Epoch 2:  86%|████████▌ | 938/1095 [00:51<00:08, 18.21it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/157 [00:00<01:00,  2.60it/s]\u001b[A\n",
      "Epoch 2:  86%|████████▌ | 944/1095 [00:52<00:08, 18.15it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Epoch 2:  87%|████████▋ | 952/1095 [00:52<00:07, 18.25it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Validating:  10%|▉         | 15/157 [00:00<00:04, 31.00it/s]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 960/1095 [00:52<00:07, 18.35it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Epoch 2:  88%|████████▊ | 968/1095 [00:52<00:06, 18.44it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Validating:  20%|█▉        | 31/157 [00:01<00:03, 39.56it/s]\u001b[A\n",
      "Epoch 2:  89%|████████▉ | 976/1095 [00:52<00:06, 18.52it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Epoch 2:  90%|████████▉ | 984/1095 [00:52<00:05, 18.59it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Validating:  29%|██▉       | 46/157 [00:01<00:02, 37.29it/s]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 992/1095 [00:53<00:05, 18.67it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Validating:  35%|███▌      | 55/157 [00:01<00:02, 39.42it/s]\u001b[A\n",
      "Epoch 2:  91%|█████████▏| 1000/1095 [00:53<00:05, 18.75it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Epoch 2:  92%|█████████▏| 1008/1095 [00:53<00:04, 18.84it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Validating:  45%|████▍     | 70/157 [00:02<00:02, 43.28it/s]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 1016/1095 [00:53<00:04, 18.93it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Epoch 2:  94%|█████████▎| 1024/1095 [00:53<00:03, 19.02it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Validating:  55%|█████▍    | 86/157 [00:02<00:01, 44.26it/s]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 1032/1095 [00:54<00:03, 19.09it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Validating:  61%|██████    | 96/157 [00:02<00:01, 40.99it/s]\u001b[A\n",
      "Epoch 2:  95%|█████████▍| 1040/1095 [00:54<00:02, 19.17it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Epoch 2:  96%|█████████▌| 1048/1095 [00:54<00:02, 19.27it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Epoch 2:  96%|█████████▋| 1056/1095 [00:54<00:02, 19.37it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Validating:  76%|███████▌  | 119/157 [00:03<00:00, 49.21it/s]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 1064/1095 [00:54<00:01, 19.46it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Epoch 2:  98%|█████████▊| 1072/1095 [00:54<00:01, 19.54it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Validating:  87%|████████▋ | 136/157 [00:03<00:00, 49.39it/s]\u001b[A\n",
      "Epoch 2:  99%|█████████▊| 1080/1095 [00:55<00:00, 19.63it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Epoch 2:  99%|█████████▉| 1088/1095 [00:55<00:00, 19.72it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0849, val_acc_epoch=0.973, val_loss_step=0.00323, val_acc_step=1.000]\n",
      "Epoch 2: 100%|██████████| 1095/1095 [00:55<00:00, 19.74it/s, loss=0.0616, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  86%|████████▌ | 938/1095 [01:00<00:10, 15.58it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  86%|████████▌ | 944/1095 [01:00<00:09, 15.56it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  87%|████████▋ | 952/1095 [01:00<00:09, 15.66it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  88%|████████▊ | 960/1095 [01:00<00:08, 15.75it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Validating:  14%|█▍        | 22/157 [00:00<00:03, 40.48it/s]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 968/1095 [01:01<00:08, 15.85it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  89%|████████▉ | 976/1095 [01:01<00:07, 15.95it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  90%|████████▉ | 984/1095 [01:01<00:06, 16.04it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  91%|█████████ | 992/1095 [01:01<00:06, 16.14it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Validating:  34%|███▍      | 54/157 [00:01<00:01, 56.24it/s]\u001b[A\n",
      "Epoch 3:  91%|█████████▏| 1000/1095 [01:01<00:05, 16.23it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  92%|█████████▏| 1008/1095 [01:01<00:05, 16.33it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  93%|█████████▎| 1016/1095 [01:01<00:04, 16.42it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  94%|█████████▎| 1024/1095 [01:01<00:04, 16.52it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  94%|█████████▍| 1032/1095 [01:02<00:03, 16.61it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  95%|█████████▍| 1040/1095 [01:02<00:03, 16.70it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Validating:  66%|██████▌   | 103/157 [00:02<00:00, 57.92it/s]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 1048/1095 [01:02<00:02, 16.79it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  96%|█████████▋| 1056/1095 [01:02<00:02, 16.88it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  97%|█████████▋| 1064/1095 [01:02<00:01, 16.97it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  98%|█████████▊| 1072/1095 [01:02<00:01, 17.06it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Validating:  85%|████████▌ | 134/157 [00:02<00:00, 56.77it/s]\u001b[A\n",
      "Epoch 3:  99%|█████████▊| 1080/1095 [01:02<00:00, 17.14it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3:  99%|█████████▉| 1088/1095 [01:03<00:00, 17.24it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0806, val_acc_epoch=0.975, val_loss_step=0.000297, val_acc_step=1.000]\n",
      "Epoch 3: 100%|██████████| 1095/1095 [01:03<00:00, 17.26it/s, loss=0.0618, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000] \n",
      "Epoch 4:  86%|████████▌ | 938/1095 [00:54<00:09, 17.11it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  86%|████████▌ | 944/1095 [00:55<00:08, 17.07it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  87%|████████▋ | 952/1095 [00:55<00:08, 17.18it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Validating:   9%|▉         | 14/157 [00:00<00:04, 31.00it/s]\u001b[A\n",
      "Epoch 4:  88%|████████▊ | 960/1095 [00:55<00:07, 17.29it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  88%|████████▊ | 968/1095 [00:55<00:07, 17.39it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  89%|████████▉ | 976/1095 [00:55<00:06, 17.49it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  90%|████████▉ | 984/1095 [00:55<00:06, 17.59it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Validating:  30%|██▉       | 47/157 [00:01<00:02, 53.92it/s]\u001b[A\n",
      "Epoch 4:  91%|█████████ | 992/1095 [00:56<00:05, 17.69it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  91%|█████████▏| 1000/1095 [00:56<00:05, 17.79it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  92%|█████████▏| 1008/1095 [00:56<00:04, 17.89it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  93%|█████████▎| 1016/1095 [00:56<00:04, 17.98it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Validating:  50%|████▉     | 78/157 [00:01<00:01, 55.94it/s]\u001b[A\n",
      "Epoch 4:  94%|█████████▎| 1024/1095 [00:56<00:03, 18.08it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  94%|█████████▍| 1032/1095 [00:56<00:03, 18.17it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  95%|█████████▍| 1040/1095 [00:56<00:03, 18.27it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Validating:  65%|██████▍   | 102/157 [00:02<00:00, 56.66it/s]\u001b[A\n",
      "Epoch 4:  96%|█████████▌| 1048/1095 [00:57<00:02, 18.36it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  96%|█████████▋| 1056/1095 [00:57<00:02, 18.46it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  97%|█████████▋| 1064/1095 [00:57<00:01, 18.56it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Validating:  80%|████████  | 126/157 [00:02<00:00, 57.65it/s]\u001b[A\n",
      "Epoch 4:  98%|█████████▊| 1072/1095 [00:57<00:01, 18.65it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  99%|█████████▊| 1080/1095 [00:57<00:00, 18.75it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4:  99%|█████████▉| 1088/1095 [00:57<00:00, 18.85it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0742, val_acc_epoch=0.976, val_loss_step=0.00069, val_acc_step=1.000]\n",
      "Epoch 4: 100%|██████████| 1095/1095 [00:57<00:00, 18.88it/s, loss=0.0405, v_num=0, val_loss_epoch=0.0616, val_acc_epoch=0.982, val_loss_step=0.00221, val_acc_step=1.000]\n",
      "  0%|          | 0/157 [00:00<?, ?it/s]\n",
      "100%|██████████| 157/157 [00:05<00:00, 29.70it/s]\n",
      "Validation Accuracy:  98.17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "IMG_SZ = 28\n",
    "IMG_CHANNELS = 1\n",
    "NUM_CLASSES = 10\n",
    "NUM_PATCHES = 4\n",
    "NUM_CHANNELS = 128\n",
    "DEPTH = 10\n",
    "EXPANSION = 4\n",
    "EPOCHS = 5\n",
    "\n",
    "model = MLPMixer(\n",
    "    IMG_SZ,\n",
    "    img_channels=IMG_CHANNELS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    depth=DEPTH,\n",
    "    num_patches=NUM_PATCHES,\n",
    "    num_channels=NUM_CHANNELS,\n",
    "    expansion=EXPANSION,\n",
    ")\n",
    "\n",
    "print(summary(model, input_size=(1, IMG_CHANNELS, IMG_SZ, IMG_SZ)))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=\"logs\",\n",
    "    gpus=(1 if torch.cuda.is_available() else 0),\n",
    "    max_epochs=EPOCHS,\n",
    "    logger=pl.loggers.TensorBoardLogger(\"logs/\", name=\"mlp_mixer\", version=0),\n",
    "    precision=16,\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader=data.train_dl, val_dataloaders=data.val_dl)\n",
    "\n",
    "model = model.eval()\n",
    "ys = []\n",
    "outs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(data.val_dl):\n",
    "        out = model(x).detach()\n",
    "        outs.append(out.reshape(x.shape[0], -1))\n",
    "        ys.append(y)\n",
    "\n",
    "outs = torch.cat(outs, dim=0)\n",
    "labels = torch.cat(ys, dim=0)\n",
    "\n",
    "labels_hat = torch.argmax(outs, dim=1)\n",
    "val_acc = torch.sum(labels == labels_hat).item() / (len(labels) * 1.0)\n",
    "\n",
    "print(\"\\nValidation Accuracy: \", round(100*val_acc, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}