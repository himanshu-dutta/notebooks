{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0462ec864d8fbee2e59fe671617ad28fa929004513ffead2e29601211bfa9e0b7",
   "display_name": "Python 3.8.5 64-bit ('.env': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "462ec864d8fbee2e59fe671617ad28fa929004513ffead2e29601211bfa9e0b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import collections\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, dataloader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "VAL_SIZE = 0.3\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-4\n",
    "EPOCHS = 50\n",
    "VERSION = 0\n",
    "\n",
    "CHECKPOINT = \"./checkpoints/sot_0.ckpt\""
   ]
  },
  {
   "source": [
    "# Utils"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image):\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_examples(images, bboxes=None):\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    columns = 4\n",
    "    rows = 4\n",
    "\n",
    "    for i in range(1, len(images) + 1):\n",
    "        if bboxes is not None:\n",
    "            img = visualize_bbox(images[i - 1], bboxes[i - 1], class_name=\"Elon\")\n",
    "        else:\n",
    "            img = images[i - 1]\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(img.astype(np.uint8))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# From https://albumentations.ai/docs/examples/example_bboxes/\n",
    "def visualize_bbox(img, bbox, class_name, color=(255, 0, 0), thickness=5):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, x_max, y_max = map(int, bbox)\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, thickness)\n",
    "    return img\n"
   ]
  },
  {
   "source": [
    "# Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rectangle = collections.namedtuple(\"Rectangle\", [\"x\", \"y\", \"width\", \"height\"])\n",
    "Point = collections.namedtuple(\"Point\", [\"x\", \"y\"])\n",
    "Polygon = collections.namedtuple(\"Polygon\", [\"points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_region(region, to):\n",
    "\n",
    "    if to == \"rectangle\":\n",
    "\n",
    "        if isinstance(region, Rectangle):\n",
    "            return copy.copy(region)\n",
    "        elif isinstance(region, Polygon):\n",
    "            top = sys.float_info.max\n",
    "            bottom = sys.float_info.min\n",
    "            left = sys.float_info.max\n",
    "            right = sys.float_info.min\n",
    "\n",
    "            for point in region.points:\n",
    "                top = min(top, point.y)\n",
    "                bottom = max(bottom, point.y)\n",
    "                left = min(left, point.x)\n",
    "                right = max(right, point.x)\n",
    "\n",
    "            return Rectangle(left, top, right - left, bottom - top)\n",
    "\n",
    "        else:\n",
    "            return None\n",
    "    if to == \"polygon\":\n",
    "\n",
    "        if isinstance(region, Rectangle):\n",
    "            points = []\n",
    "            points.append((region.x, region.y))\n",
    "            points.append((region.x + region.width, region.y))\n",
    "            points.append((region.x + region.width, region.y + region.height))\n",
    "            points.append((region.x, region.y + region.height))\n",
    "            return Polygon(points)\n",
    "\n",
    "        elif isinstance(region, Polygon):\n",
    "            return copy.copy(region)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bbox(bbox):\n",
    "    if len(bbox) == 4:\n",
    "        x1, y1, w, h = bbox\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "        return [x1, y1, x2, y2]\n",
    "\n",
    "    elif len(bbox) > 4:\n",
    "        pts = []\n",
    "        for idx in range(0, len(bbox), 2):\n",
    "            pts.append(Point(bbox[idx], bbox[idx + 1]))\n",
    "        poly = Polygon(pts)\n",
    "        rect = convert_region(poly, \"rectangle\")\n",
    "        x1, y1, w, h = rect.x, rect.y, rect.width, rect.height\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "        return [x1, y1, x2, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, data_dir: Path = Path(DATA_DIR)):\n",
    "\n",
    "        objects = [\n",
    "            obj\n",
    "            for obj in list(data_dir.glob(\"*\"))\n",
    "            if obj.is_dir() and not str(obj.name).startswith(\".\")\n",
    "        ]\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for obj in objects:\n",
    "            img_path = obj / \"color\"\n",
    "            annot_path = obj / \"groundtruth.txt\"\n",
    "\n",
    "            images = natsorted(list(img_path.glob(\"*\")))\n",
    "\n",
    "            with open(str(annot_path), \"r\") as fl:\n",
    "                annots = fl.read()\n",
    "                annots = annots.split(\"\\n\")\n",
    "                annots = [\n",
    "                    [float(coord) for coord in annot.split(\",\")]\n",
    "                    for annot in annots\n",
    "                    if annot != \"\"\n",
    "                ]\n",
    "\n",
    "            annots = list(map(convert_to_bbox, annots))\n",
    "\n",
    "            data += list(\n",
    "                zip(\n",
    "                    images[:-1],\n",
    "                    annots[:-1],\n",
    "                    images[1:],\n",
    "                    annots[1:],\n",
    "                    [obj.name] * (len(images) - 1),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.data = data\n",
    "        self.transform_x = A.Compose(\n",
    "            [\n",
    "                A.RandomCropNearBBox(always_apply=True),\n",
    "                A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "                ToTensorV2(),\n",
    "            ],\n",
    "            p=1.0,\n",
    "        )\n",
    "\n",
    "        self.transform_y = A.Compose(\n",
    "            [\n",
    "                A.RandomSizedBBoxSafeCrop(IMG_SIZE, IMG_SIZE),\n",
    "                ToTensorV2(),\n",
    "            ],\n",
    "            p=1.0,\n",
    "            bbox_params=A.BboxParams(\n",
    "                format=\"pascal_voc\", label_fields=[], min_visibility=0.3\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_x, bbox_x, image_y, bbox_y, obj = self.data[index]\n",
    "\n",
    "        image_x = np.array(Image.open(image_x))\n",
    "        bbox_x = np.array(bbox_x, dtype=np.float32)\n",
    "\n",
    "        image_y = np.array(Image.open(image_y))\n",
    "        bbox_y = np.array(bbox_y, dtype=np.float32)\n",
    "\n",
    "        try:\n",
    "            if self.transform_x:\n",
    "                transformed = self.transform_x(image=image_x, cropping_bbox=bbox_x)\n",
    "                image_x = transformed[\"image\"]\n",
    "\n",
    "            if self.transform_y:\n",
    "                transformed = self.transform_y(image=image_y, bboxes=[bbox_y])\n",
    "                image_y = transformed[\"image\"]\n",
    "                bbox_y = torch.tensor(transformed[\"bboxes\"][0]).float()\n",
    "\n",
    "            return {\n",
    "                \"previous_frame\": image_x.float(),  # previous frame\n",
    "                \"current_frame\": image_y.float(),  # current frame\n",
    "                \"bbox\": bbox_y,  # target\n",
    "                \"name\": obj,  # object name\n",
    "            }\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return dataloader.default_collate(batch)\n",
    "\n",
    "\n",
    "def reverse_transform(\n",
    "    img: torch.Tensor,\n",
    "    bbox: torch.Tensor,\n",
    "    width: int = None,\n",
    "    height: int = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Used to convert back from the processed image size and bounding box to original size and corresponding bounding box.\n",
    "    \"\"\"\n",
    "    if width == None:\n",
    "        width = img.shape[1]\n",
    "    if height == None:\n",
    "        height = img.shape[2]\n",
    "\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    bbox = bbox.numpy()\n",
    "\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(width, height),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"pascal_voc\", label_fields=[], min_visibility=0.3\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    transformed = transform(image=img, bboxes=[bbox])\n",
    "\n",
    "    return transformed[\"image\"], transformed[\"bboxes\"][0]"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOTModel(pl.LightningModule):\n",
    "    def __init__(self, lr=LEARNING_RATE):\n",
    "        super(SOTModel, self).__init__()\n",
    "\n",
    "        self.x_cnn = nn.Sequential(\n",
    "            *(list(models.resnet34(pretrained=True).children())[:-1])\n",
    "        )\n",
    "        self.y_cnn = nn.Sequential(\n",
    "            *(list(models.resnet34(pretrained=True).children())[:-1])\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 2, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 4),\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, previous_frame, current_frame):\n",
    "        x_feature = self.x_cnn(previous_frame)\n",
    "        y_feature = self.y_cnn(current_frame)\n",
    "\n",
    "        x_feature = self.flatten(x_feature)\n",
    "        y_feature = self.flatten(y_feature)\n",
    "\n",
    "        features = torch.cat([x_feature, y_feature], dim=1)\n",
    "\n",
    "        return self.sigmoid_scale(self.fc(features), 0, IMG_SIZE)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        target = batch[\"bbox\"]\n",
    "\n",
    "        out = self(batch[\"previous_frame\"], batch[\"current_frame\"])\n",
    "        return self.loss(out, target)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        target = batch[\"bbox\"]\n",
    "\n",
    "        out = self(batch[\"previous_frame\"], batch[\"current_frame\"])\n",
    "        self.log(\"val_mse\", self.loss(out, target), on_step=True, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        target = batch[\"bbox\"]\n",
    "\n",
    "        out = self(batch[\"previous_frame\"], batch[\"current_frame\"])\n",
    "        self.log(\"test_mse\", self.loss(out, target), on_step=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), self.lr)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_scale(x, lo, hi):\n",
    "        return torch.sigmoid(x) * (hi - lo) + lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval():\n",
    "    model = SOTModel()\n",
    "    model = model.load_from_checkpoint(CHECKPOINT).eval()\n",
    "    print(\"Model loaded successfully...\")\n",
    "\n",
    "    ds = Data()\n",
    "    split_idx = int(len(ds) * VAL_SIZE)\n",
    "    indices = list(range(len(ds)))\n",
    "\n",
    "    train_indices, val_indices = indices[:split_idx], indices[split_idx:]\n",
    "    train_sampler, val_sampler = SubsetRandomSampler(\n",
    "        train_indices\n",
    "    ), SubsetRandomSampler(val_indices)\n",
    "\n",
    "    val_dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size=8,\n",
    "        sampler=val_sampler,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    val_batch = next(iter(val_dl))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(val_batch[\"previous_frame\"], val_batch[\"current_frame\"])\n",
    "\n",
    "    org_imgs = []\n",
    "    pred_imgs = []\n",
    "    org_bboxes = []\n",
    "    pred_bboxes = []\n",
    "\n",
    "    imgs = []\n",
    "    bboxes = []\n",
    "\n",
    "    for idx in range(len(out)):\n",
    "        org_img, org_bbox = reverse_transform(\n",
    "            val_batch[\"current_frame\"][idx],\n",
    "            val_batch[\"bbox\"][idx],\n",
    "            480,\n",
    "            720,\n",
    "        )\n",
    "\n",
    "        pred_img, pred_bbox = reverse_transform(\n",
    "            val_batch[\"current_frame\"][idx],\n",
    "            out[idx],\n",
    "            480,\n",
    "            720,\n",
    "        )\n",
    "\n",
    "        imgs += [org_img, pred_img]\n",
    "        bboxes += [org_bbox, pred_bbox]\n",
    "\n",
    "    plot_examples(imgs, bboxes)"
   ]
  },
  {
   "source": [
    "# Execution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = Data()\n",
    "val_sz = int(len(ds) * VAL_SIZE)\n",
    "test_sz = int(len(ds) * TEST_SIZE)\n",
    "train_sz = len(ds) - val_sz - test_sz\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(ds, [train_sz, val_sz, test_sz])\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count(),\n",
    "    collate_fn=collate,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=os.cpu_count(),\n",
    "    collate_fn=collate,\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=os.cpu_count(),\n",
    "    collate_fn=collate,\n",
    ")\n",
    "\n",
    "model = SOTModel()\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=\"logs\",\n",
    "    gpus=(1 if torch.cuda.is_available() else 0),\n",
    "    max_epochs=EPOCHS,\n",
    "    precision=16,\n",
    "    logger=pl.loggers.TensorBoardLogger(\"logs/\", name=\"sot\", version=VERSION),\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloader=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    ")\n",
    "trainer.test(test_dataloaders=test_dl)\n",
    "\n",
    "trainer.save_checkpoint(f\"checkpoints/sot_{VERSION}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}